#!/usr/bin/env python3
"""
Phase 0: TinySwallow-1.5B MLXç‰ˆã®å‹•ä½œç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆ
M4 MacBook Air 16GB ã§ã®æ¨è«–ãƒ†ã‚¹ãƒˆ
"""

import time
import psutil
import sys
from pathlib import Path

def check_system():
    """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ç¢ºèª"""
    print("ğŸ” ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ç¢ºèª")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {sys.platform}")
    
    # ãƒ¡ãƒ¢ãƒªæƒ…å ±
    memory = psutil.virtual_memory()
    print(f"ç·ãƒ¡ãƒ¢ãƒª: {memory.total / (1024**3):.1f} GB")
    print(f"åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {memory.available / (1024**3):.1f} GB")
    print("-" * 50)

def test_mlx_installation():
    """MLXãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª"""
    print("ğŸ“¦ MLXãƒ©ã‚¤ãƒ–ãƒ©ãƒªç¢ºèª")
    try:
        import mlx.core as mx
        print("âœ… MLX core: OK")
        
        # ç°¡å˜ãªå‹•ä½œãƒ†ã‚¹ãƒˆï¼ˆæµ®å‹•å°æ•°ç‚¹å‹ã‚’ä½¿ç”¨ï¼‰
        a = mx.array([1.0, 2.0, 3.0, 4.0])  # floatå‹ã«å¤‰æ›´
        b = mx.array([5.0, 6.0, 7.0, 8.0])  # floatå‹ã«å¤‰æ›´
        
        # MLXã§ã¯@æ¼”ç®—å­ã¾ãŸã¯matmulã‚’ä½¿ç”¨ï¼ˆfloatå‹ã®ã¿ã‚µãƒãƒ¼ãƒˆï¼‰
        c = a @ b  # ã¾ãŸã¯ mx.matmul(a, b)
        print(f"âœ… MLXåŸºæœ¬å‹•ä½œãƒ†ã‚¹ãƒˆ (å†…ç©): {c.item()}")
        
        # ä»–ã®åŸºæœ¬æ¼”ç®—ã‚‚ãƒ†ã‚¹ãƒˆï¼ˆæ•´æ•°å‹ã¯sumç­‰ã§ã¯å•é¡Œãªã—ï¼‰
        a_int = mx.array([1, 2, 3, 4])
        d = mx.sum(a_int)
        print(f"âœ… MLX sum ãƒ†ã‚¹ãƒˆ: {d.item()}")
        
        # è¡Œåˆ—æ¼”ç®—ã®ãƒ†ã‚¹ãƒˆï¼ˆæµ®å‹•å°æ•°ç‚¹å‹ï¼‰
        matrix_a = mx.array([[1.0, 2.0], [3.0, 4.0]])
        matrix_b = mx.array([[5.0, 6.0], [7.0, 8.0]])
        matrix_c = matrix_a @ matrix_b
        print(f"âœ… MLXè¡Œåˆ—ä¹—ç®—ãƒ†ã‚¹ãƒˆ: æ­£å¸¸å‹•ä½œ")
        print(f"   çµæœä¾‹: {matrix_c[0, 0].item()}")
        
    except ImportError as e:
        print(f"âŒ MLX core ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False
        
    try:
        from mlx_lm import load, generate
        print("âœ… MLX-LM: OK")
        return True
    except ImportError as e:
        print(f"âŒ MLX-LM ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

def download_and_test_tinyswallow():
    """TinySwallow-1.5B MLXç‰ˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ¦¢ TinySwallow-1.5B-Instruct MLXç‰ˆãƒ†ã‚¹ãƒˆ")
    
    try:
        from mlx_lm import load, generate
        
        # MLXå½¢å¼ã®TinySwallow-1.5B-Instructãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        print("ğŸ“¥ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        model_name = "mlx-community/TinySwallow-1.5B-Instruct-4bit"
        
        start_time = time.time()
        model, tokenizer = load(model_name)
        load_time = time.time() - start_time
        
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†: {load_time:.2f}ç§’")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
        memory = psutil.virtual_memory()
        used_memory = (memory.total - memory.available) / (1024**3)
        print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¾Œãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {used_memory:.1f} GB")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return None, None

def test_conversation(model, tokenizer):
    """å®Ÿéš›ã®ä¼šè©±ãƒ†ã‚¹ãƒˆ"""
    if model is None or tokenizer is None:
        print("âŒ ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return
        
    print("\nğŸ’¬ ä¼šè©±ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    test_prompts = [
        "ã“ã‚“ã«ã¡ã¯ï¼ã‚ãªãŸã¯èª°ã§ã™ã‹ï¼Ÿ",
        "ç§ã®è¶£å‘³ã¯èª­æ›¸ã§ã™ã€‚ãŠã™ã™ã‚ã®æœ¬ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
        "ä»Šæ—¥ã¯å¤©æ°—ãŒè‰¯ã„ã§ã™ã­ã€‚"
    ]
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\n--- ãƒ†ã‚¹ãƒˆ {i} ---")
        print(f"ğŸ‘¤ è³ªå•: {prompt}")
        
        try:
            # MLX-LM æœ€æ–°APIä½¿ç”¨ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
            from mlx_lm import generate
            
            # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨
            if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template is not None:
                messages = [{"role": "user", "content": prompt}]
                formatted_prompt = tokenizer.apply_chat_template(
                    messages, 
                    add_generation_prompt=True,
                    tokenize=False
                )
            else:
                formatted_prompt = prompt
            
            # æ¨è«–å®Ÿè¡Œï¼ˆåŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ï¼‰
            start_time = time.time()
            response = generate(
                model, 
                tokenizer, 
                prompt=formatted_prompt, 
                verbose=False,
                max_tokens=100
                # æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ç¾åœ¨ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“
            )
            inference_time = time.time() - start_time
            
            print(f"ğŸ¤– å›ç­”: {response}")
            print(f"â±ï¸  æ¨è«–æ™‚é–“: {inference_time:.2f}ç§’")
            
        except Exception as e:
            print(f"âŒ æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
            print(f"   ãƒ‡ãƒãƒƒã‚°: model type = {type(model)}")
            print(f"   ãƒ‡ãƒãƒƒã‚°: tokenizer type = {type(tokenizer)}")

def benchmark_performance(model, tokenizer):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    if model is None or tokenizer is None:
        return
        
    print("\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ")
    
    prompt = "äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"
    
    # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨
    if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template is not None:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            add_generation_prompt=True,
            tokenize=False
        )
    else:
        formatted_prompt = prompt
    
    try:
        from mlx_lm import generate
        
        start_time = time.time()
        response = generate(
            model, 
            tokenizer, 
            prompt=formatted_prompt, 
            verbose=True,  # ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆé€Ÿåº¦ã‚’è¡¨ç¤º
            max_tokens=200
            # æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ç¾åœ¨ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“
        )
        total_time = time.time() - start_time
        
        # æ¦‚ç®—ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆæ—¥æœ¬èªã¯è¤‡é›‘ãªã®ã§æ¦‚ç®—ï¼‰
        estimated_tokens = len(response) // 2  # æ—¥æœ¬èªã®å¹³å‡çš„ãªè¦‹ç©ã‚‚ã‚Š
        tokens_per_second = estimated_tokens / total_time if total_time > 0 else 0
        
        print(f"\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµæœ:")
        print(f"ç·å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
        print(f"æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°: {estimated_tokens}")
        print(f"æ¨å®šé€Ÿåº¦: {tokens_per_second:.1f} tokens/sec")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        memory = psutil.virtual_memory()
        used_memory = (memory.total - memory.available) / (1024**3)
        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {used_memory:.1f} GB")
        
    except Exception as e:
        print(f"âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"   ç¾åœ¨ã®MLX-LMã§ã¯æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ TinySwallow-1.5B MLX å‹•ä½œç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    
    # ã‚·ã‚¹ãƒ†ãƒ ç¢ºèª
    check_system()
    
    # MLXã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
    if not test_mlx_installation():
        print("\nâŒ MLXãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        print("å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: pip install mlx mlx-lm")
        return
    
    # TinySwallowãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒ†ã‚¹ãƒˆ
    model, tokenizer = download_and_test_tinyswallow()
    
    if model is not None:
        # ä¼šè©±ãƒ†ã‚¹ãƒˆ
        test_conversation(model, tokenizer)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        benchmark_performance(model, tokenizer)
        
        print("\nğŸ‰ Phase 0 ç’°å¢ƒæ§‹ç¯‰ãƒ»æ¤œè¨¼å®Œäº†ï¼")
        print("âœ… TinySwallow-1.5B MLXç‰ˆãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")
        print("âœ… æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: Phase 1 MVPé–‹ç™ºã«é€²ã‚€ã“ã¨ãŒã§ãã¾ã™")
    else:
        print("\nâŒ ç’°å¢ƒæ§‹ç¯‰ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    main()